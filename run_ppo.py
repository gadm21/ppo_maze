"""
Loads a pre-trained PPO agent model and visualizes its trajectory in the GridEnv.

This script assumes 'ppo_model.pth' exists in the same directory, having been
generated by 'train_ppo.py'. It runs one episode of the agent interacting
with the environment, prints the steps taken, and saves a plot of the
trajectory to 'trajectory.png'.
"""

import torch as pt
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import time # For pausing
import os
import random
import argparse

# Import the MyGrid and PolicyValueNet classes from train_ppo_2
from train_ppo import GridEnv, PolicyValueNet
from torch.distributions import Categorical

def run_and_visualize(model_path='ppo_model.pth', map_file='maps/map1.txt', max_steps=50, pause_time=0.05):
    """Loads a trained PPO model, runs one episode, and visualizes the trajectory in real-time.

    Args:
        model_path (str): Path to the saved model weights.
        map_file (str): Path to the map file for the environment.
        max_steps (int): Maximum steps allowed for the episode.
        pause_time (float): Time in seconds to pause between steps for visualization.
    """
    # Set device (CPU or GPU)
    device = pt.device("cuda" if pt.cuda.is_available() else "cpu")

    # Initialize environment
    try:
        env = GridEnv(map_file=map_file, max_steps=max_steps)
    except FileNotFoundError as e:
        print(f"Error initializing environment: {e}")
        print(f"Ensure the map file exists at '{os.path.abspath(map_file)}'")
        return
    except ValueError as e:
        print(f"Error initializing environment: {e}")
        return

    # Extract the image shape and num_actions correctly from the Tuple space
    image_shape = env.observation_space.shape
    input_channels = image_shape[0] # Should be 3
    num_actions = env.action_space.n

    print("Environment height: ", env.height)
    print("Environment width: ", env.width)

    # Initialize the model
    model = PolicyValueNet(input_channels=input_channels,
                         num_actions=num_actions,
                         grid_height=env.height, 
                         grid_width=env.width)

    # --- Load Model Weights --- #
    try:
        model.load_state_dict(pt.load(model_path, map_location=device))
        model.eval() # Set model to evaluation mode
        print(f"Model loaded successfully from {model_path}")
    except FileNotFoundError:
        print(f"Error: Model file not found at {model_path}")
        print("Please train the model first by running train_ppo.py")
        return
    except Exception as e:
        print(f"Error loading model state_dict: {e}")
        print("Ensure the model architecture in run_ppo.py matches the trained model.")
        return

    print("\nRunning episode...")
    observation = env.reset()
    done = False
    trajectory = [env.agent_pos] # Store agent positions as tuples
    total_reward = 0
    steps = 0

    # --- Real-time Visualization Setup --- #
    plt.ion() # Turn on interactive mode
    fig, ax = plt.subplots()

    # Create a colormap: 0: empty, 1: wall, 2: target, 3: agent
    cmap = mcolors.ListedColormap(['white', 'black', 'green', 'blue'])
    bounds = [-0.5, 0.5, 1.5, 2.5, 3.5]
    norm = mcolors.BoundaryNorm(bounds, cmap.N)

    # Prepare grid data (static elements: walls, target)
    grid_display = np.zeros((env.height, env.width), dtype=int)
    for r, c in env.wall_positions:
        grid_display[r, c] = 1 # Wall
    grid_display[env.target_pos[0], env.target_pos[1]] = 2 # Target

    # Initial plot
    current_pos = env.agent_pos
    grid_display[current_pos[0], current_pos[1]] = 3 # Agent
    img = ax.imshow(grid_display, cmap=cmap, norm=norm)

    # Plot grid lines
    ax.set_xticks(np.arange(-.5, env.width, 1), minor=True)
    ax.set_yticks(np.arange(-.5, env.height, 1), minor=True)
    ax.grid(which="minor", color="grey", linestyle='-', linewidth=1)
    ax.tick_params(which="minor", size=0)
    ax.set_xticks(np.arange(0, env.width, 1))
    ax.set_yticks(np.arange(0, env.height, 1))
    ax.set_xticklabels([])
    ax.set_yticklabels([])

    # Plot trajectory line (initially empty)
    line, = ax.plot([], [], 'r-', marker='o', markersize=4, linewidth=2) # Red line for trajectory

    plt.title(f"Step: {steps}")
    fig.canvas.draw() # Initial draw
    fig.canvas.flush_events()
    time.sleep(pause_time)

    print("Step | Position (row, col) | Action")
    print("-----|---------------------|-------")

    # --- Episode Loop --- #
    while not done:
        image_obs = observation
        image_tensor = pt.FloatTensor(image_obs).unsqueeze(0).to(device)

        # --- Get Action from Policy --- #
        action_logits, _ = model(image_tensor)
        action_probs = pt.softmax(action_logits, dim=-1)
        action_dist = pt.distributions.Categorical(action_probs)
        action_int = action_dist.sample().item()

        action_map = {0: 'up', 1: 'down', 2: 'left', 3: 'right'}
        print(f" {steps:3d}  | {action_map.get(action_int, 'unknown')}")

        # --- Step Environment --- #
        next_observation, reward, done, _ = env.step(action_int)
        total_reward += reward
        trajectory.append(env.agent_pos)
        observation = next_observation
        steps += 1

        # --- Update Visualization --- #
        grid_display[current_pos[0], current_pos[1]] = 0
        current_pos = env.agent_pos
        grid_display[current_pos[0], current_pos[1]] = 3
        img.set_data(grid_display)
        traj_array = np.array(trajectory)
        line.set_data(traj_array[:, 1], traj_array[:, 0])
        ax.set_title(f"Step: {steps}")
        fig.canvas.draw()
        fig.canvas.flush_events()
        time.sleep(pause_time)

    # --- Episode End --- #
    if np.array_equal(env.agent_pos, env.target_pos):
        print("\nTarget reached!")
    elif steps >= max_steps:
        print("\nEpisode finished: Max steps reached.")
    else:
        print("\nEpisode finished for other reasons.")

    print(f"Total Reward: {total_reward:.2f}, Steps: {steps}")

    # Keep plot open
    plt.ioff() # Turn off interactive mode
    plt.title(f"Final Trajectory - Steps: {steps}, Reward: {total_reward:.2f}")
    plt.show()

# --- Main Execution --- #
if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="Run and visualize a trained PPO agent.")
    parser.add_argument('--map', type=str, default="no", 
                        help='Path to the map file for the environment.')
    parser.add_argument('--model', type=str, default='ppo_model.pth',
                        help='Path to the trained PPO model file.')
    args = parser.parse_args()

    # Define paths relative to the script location
    script_dir = os.path.dirname(__file__)
    model_path = os.path.join(script_dir, args.model)
    TEST_MAPS = [os.path.join(script_dir, 'maps', 'test_map_1.txt'),
                 os.path.join(script_dir, 'maps', 'test_map_2.txt'),
                 os.path.join(script_dir, 'maps', 'test_map_3.txt')]
    if args.map == "no":
        map_file = random.choice(TEST_MAPS)
    else:
        map_file = args.map
    print(f"Using map: {map_file}")


    run_and_visualize(model_path=model_path, map_file=map_file)