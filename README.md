# PPO Agent for Grid World Navigation

This project implements a Proximal Policy Optimization (PPO) agent to solve a simple navigation task in a 2D grid world. The agent learns to navigate from a starting point to a target position while avoiding obstacles in a grid environment. The implementation uses deep reinforcement learning with an enhanced neural network architecture including convolutional layers, residual connections, and separate processing paths for spatial and positional data.

## Files

*   `train_ppo.py`: Contains the code for the PPO algorithm, the enhanced Actor-Critic neural network (`PolicyValueNet`), the `GridEnv` environment, and the main training loop. Running this script trains the agent and saves the model weights to `ppo_model.pth`.
*   `run_ppo.py`: Loads the pre-trained `ppo_model.pth` and runs the agent for one episode in the environment, visualizing the resulting trajectory in real-time with an anti-looping mechanism.
*   `maps/map1.txt`: Contains the map definition with start (S), target (T), walls (W), and empty spaces (E).
*   `requirements.txt`: Lists the necessary Python packages.
*   `ppo_model.pth`: (Generated by `train_ppo.py`) Stores the trained model weights with the best average reward during training.

## Setup

1.  **Clone the repository or download the files.**
2.  **Create a Python virtual environment (recommended):**
    ```bash
    python -m venv ppo_env
    source ppo_env/bin/activate  # On Windows use `ppo_env\Scripts\activate`
    ```
3.  **Install the required packages:**
    ```bash
    pip install -r requirements.txt
    ```

## Training the Agent

To train the PPO agent, run the `train_ppo.py` script:

```bash
python train_ppo.py
```

This will:
*   Initialize the environment and the `PolicyValueNet` model.
*   Run the training loop for a predefined number of episodes.
*   Collect trajectories, calculate advantages (using GAE), and update the model using the PPO clipped objective.
*   Print progress updates periodically.
*   Save the trained model's weights to `ppo_model.pth` upon completion.

## Running and Visualizing the Trained Agent

After training (which generates `ppo_model.pth`), you can visualize the agent's behavior by running `run_ppo.py`:

```bash
# Ensure your virtual environment is active
# (if you used the full path before, use that:) /path/to/ppo_env/bin/python run_ppo.py 
python run_ppo.py
```

This will:
*   Load the weights from `ppo_model.pth` into the `PolicyValueNet`.
*   Run the agent for one episode in the `GridEnv`.
*   Print the steps taken (position and action) during the episode.
*   Visualize the agent's path in real-time with an interactive plot.
*   Automatically detect and break out of loops with random actions when the agent gets stuck.
*   Display the total reward and steps taken after the episode completes.